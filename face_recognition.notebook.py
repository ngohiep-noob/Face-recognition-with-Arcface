# -*- coding: utf-8 -*-
"""face_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ezmDW6vCCAuFcCnx5nXVr-oe6ZIGCO5

# Environment settings
"""

from google.colab import drive

drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/Advanced computer vision/Face recognition

!pip install pymongo

import cv2
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
from InsightFace_Pytorch.model import Backbone
import torch
from torchvision import transforms as T
from torch import nn
import os
from pathlib import Path
import numpy as np
from PIL import Image
import bson
from bson.objectid import ObjectId

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device

"""# Utilities"""

def show_images(img_list): # img = list(tuple(image, title))
  num_img = len(img_list)
  figsize = num_img * 10

  plt.figure(figsize = (figsize, figsize))

  for i in range(num_img):
    img, title = img_list[i]

    plt.subplot(1, num_img, i+1)
    plt.title(title)
    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = plt.imshow(img, interpolation = 'bicubic')

def drawBoundingBoxes(image, detections, color = (0,0,255)):
    """Draw bounding boxes on an image.
    imageData: image data in numpy array format
    imageOutputPath: output image file path
    inferenceResults: inference results array off object (l,t,w,h)
    colorMap: Bounding box color candidates, list of RGB tuples.
    """
    img_with_dets = image.copy()
    min_conf = 0.9
    for det in detections:
      if det['confidence'] >= min_conf:
        x, y, width, height = det['box']
        label = det['label']

        cv2.rectangle(img_with_dets, (x,y), (x+width,y+height), (0,155,255), 2)
        imgHeight, imgWidth, _ = image.shape

        thick = int((imgHeight + imgWidth) // 900)
        cv2.putText(img_with_dets, label, (x, y - 12), 0, 1e-3 * imgHeight, color, thick//3)

    plt.figure(figsize = (5,5))
    plt.imshow(img_with_dets)
    plt.axis('off')

def to_cuda(elements):
    if torch.cuda.is_available():
        return elements.cuda()
    return elements

"""# Detection"""

!pip install mtcnn

from mtcnn import MTCNN
import cv2

detector = MTCNN()

sample1 = cv2.imread('/content/gdrive/MyDrive/Advanced computer vision/Face recognition/samples/Adam_Scott/Adam_Scott_0001.jpg')

image_list = [(sample1, 'Sample 1')]
show_images(image_list)

sample1 = '/content/gdrive/MyDrive/Advanced computer vision/Face recognition/samples/Adam_Scott/Adam_Scott_0001.jpg'
img = cv2.cvtColor(cv2.imread(sample1), cv2.COLOR_BGR2RGB)

detections = detector.detect_faces(img)

detections[0]['label'] = 'Adam_Scott' # Indentification phrase

detections

img.shape

drawBoundingBoxes(img, detections)

"""# Load pretrained model"""

model = Backbone(50, 0.5, 'ir_se')

weight = torch.load('/content/gdrive/MyDrive/Advanced computer vision/Face recognition/model_ir_se50.pth')
model.load_state_dict(weight)

model = to_cuda(model)
model.eval()

print('Input layer:', model._modules['input_layer'])

print('Output layer:', model._modules['output_layer'])

from torchsummary import summary

summary(model, input_size=(3, 112, 112))

"""# Feature extraction"""

sample_path = Path('/content/gdrive/MyDrive/Advanced computer vision/Face recognition/samples')

os.listdir(sample_path)

preprocess = T.Compose([
   T.ToTensor(),
   T.Resize((112, 112),antialias=True),
   T.Normalize(
       mean=[0.485, 0.456, 0.406],
       std=[0.229, 0.224, 0.225]
   )
])
cos = to_cuda(nn.CosineSimilarity())

def get_bounding_boxes(img):
  """
  Input: image
  Output: [{ box, face }]
  """
  min_cf = 0.9
  dects = detector.detect_faces(img)

  faces = []
  face_dict = {}

  for dect in dects:
    if dect['confidence'] >= min_cf:
      x, y, w, h = dect['box']
      cropped_face = img[y:y+h, x:x+w].copy()
      faces.append({ "box": dect['box'], "face": cropped_face})

  return faces

def get_faces(image_folder):
  imgs = []
  dect_faces = []

  for filename in os.listdir(image_folder):
    file_path = image_folder / filename

    img = cv2.cvtColor(cv2.imread(str(file_path)), cv2.COLOR_BGR2RGB)

    faces = get_bounding_boxes(img)

    for f in faces:
      f['name'] = image_folder.parts[-1].split('_')[-1]

    dect_faces.extend(faces)


  return dect_faces

def embed_faces(faces_dict, feature_extractor):
  for face in faces_dict:
    img = to_cuda(preprocess(face['face']))

    img = img.reshape(1, *img.shape)

    face['embedding'] = feature_extractor(img)

  return faces_dict

scott_faces = get_faces(sample_path / 'Adam_Scott')

show_images([(face["face"], face["name"]) for face in scott_faces])

scott_faces_dict = embed_faces(scott_faces, model)

cos(scott_faces_dict[0]['embedding'], scott_faces_dict[1]['embedding'])

jubeir_faces = get_faces(sample_path / 'Adel_Al-Jubeir')

show_images([(face["face"], face["name"]) for face in jubeir_faces])

jubeir_faces_dict = embed_faces(jubeir_faces, model)

jubeir_faces_dict[0].keys()

jubeir_faces_dict[0]['embedding'].shape

cos(jubeir_faces_dict[0]['embedding'], jubeir_faces_dict[1]['embedding'])

cos(jubeir_faces_dict[0]['embedding'], jubeir_faces_dict[2]['embedding'])

cos(jubeir_faces_dict[2]['embedding'], jubeir_faces_dict[1]['embedding'])

cos(jubeir_faces_dict[0]['embedding'], scott_faces_dict[0]['embedding'])

"""# Connect to MongoDB and implement vector search"""

import pymongo
from google.colab import userdata

client = pymongo.MongoClient('mongodb+srv://hoanghiephai:3X4qobjRUBpUBcDR@face-embeddings.m9dpaia.mongodb.net/')
db = client.face_recognition

db.list_collection_names()

facebank = db.facebank
person_info = db.person_info

facebank,person_info

"""Recognition

phrase 1: hình -> lấy bbox(tọa độ, hình) -> embed_face() -> embedding(vector) -> query db -> thêm name
"""

embedding_path = 'face_embedding'

# person_info collection
pid = ObjectId()

person = {
    "_id":  pid,
    "name": "Ngo Hiep",
}

# facebank collection
face_embedding = {
    "_id":  ObjectId(),
    "personId": pid,
    "image": "", # image of cropped face
    f"{embedding_path}": np.random.rand(5) # embedding for this face, shape (512,)
}

print(person)
print(face_embedding)

def find_one_by_id(collection, id):
  # Query a document by its id
  id = ObjectId(id)

  result = collection.find_one({'_id': id})

  return result

def get_similar_faces(embbeding, limit = 5, num_candidates = 100):
  results = facebank.aggregate([
  {
      "$vectorSearch": {
        "queryVector": embbeding,
        "path": embedding_path,
        "numCandidates": num_candidates,
        "limit": limit,
        "index": "default",
        }
    }
  ])

  return results

jubeir_faces[0]['embedding'].shape

r = get_similar_faces(jubeir_faces[0]['embedding'].reshape(512,).tolist())

for i in r:
  print(i.keys())

def voting_faces(sim_faces):
  # cal avg sim based on id
  # output: tuple(id, avg_sim)[]
  sim_dict = {}

  for face in sim_faces:
    pid = face['personId']
    sim = face['']

    if sim_dict['pid']:
      sim_dict.append('')


  return

def add_person(person):
  result = person_info.insert_one(person)

  return result.inserted_id

def encode_image(image):
  return bson.binary.Binary(image)

def decode_image(bytes, target_shape, dtype = 'unit8'):
  np_arr = np.frombuffer(bytes)
  return np_arr.reshape(target_shape)

def add_face(pid, dic):
  image, embedding = dic['face'], dic['embedding']

  pid = ObjectId(pid)

  facebank.insert_one({
      'personId': pid,
      'image': encode_image(image),
      f'{embedding_path}': embedding
  })

# face_embedding = {
#     "personId": pid,
#     "image": bson.binary.Binary(jubeir_faces[0]['face']), # image of cropped face
#     f"{embedding_path}": np.random.rand(512).tolist() # embedding for this face, shape (512,)
# }

# facebank.insert_one(face_embedding)

"""# Streamlit settings"""

!pip install -q streamlit

!npm install localtunnel

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/app.py
# 
# import streamlit as st
# from PIL import Image
# import numpy as np
# import torch
# from torchvision import transforms as T
# from torch import nn
# from InsightFace_Pytorch.model import Backbone
# import bson
# from bson.objectid import ObjectId
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 
# device
# 
# model = Backbone(50, 0.5, 'ir_se')
# 
# def to_cuda(elements):
#     if torch.cuda.is_available():
#         return elements.cuda()
#     return elements
# 
# weight = torch.load('/content/gdrive/MyDrive/Advanced computer vision/Face recognition/model_ir_se50.pth')
# model.load_state_dict(weight)
# 
# model = to_cuda(model)
# model.eval()
# 
# preprocess = T.Compose([
#    T.ToTensor(),
#    T.Resize((112, 112),antialias=True),
#    T.Normalize(
#        mean=[0.485, 0.456, 0.406],
#        std=[0.229, 0.224, 0.225]
#    )
# ])
# cos = to_cuda(nn.CosineSimilarity())
# 
# import pymongo
# from google.colab import userdata
# 
# client = pymongo.MongoClient('mongodb+srv://hoanghiephai:3X4qobjRUBpUBcDR@face-embeddings.m9dpaia.mongodb.net/')
# db = client.face_recognition
# 
# db.list_collection_names()
# 
# facebank = db.facebank
# person_info = db.person_info
# 
# def embed_faces(faces_dict, feature_extractor):
#   for face in faces_dict:
#     img = to_cuda(preprocess(face['face']))
# 
#     img = img.reshape(1, *img.shape)
# 
#     face['embedding'] = feature_extractor(img)
# 
#   return faces_dict
# 
# def encode_image(image):
#   return bson.binary.Binary(image)
# 
# def decode_image(bytes, target_shape, dtype = 'unit8'):
#   np_arr = np.frombuffer(bytes)
#   return np_arr.reshape(target_shape)
# 
# embedding_path = 'face_embedding'
# 
# def register_face():
#   st.title("Register Face")
#   img_file_buffer = st.camera_input("Take a photo to register")
#   images = []
# 
#   user_type = st.selectbox("Select User Type", ["New User", "Registered User"])
#   registered_users = ["User1", "User2", "User3"]  # Replace with your actual list of registered users
# 
# 
#   if user_type == "New User":
#       new_user_name = st.text_input("Enter New Name:")
#       # save_new_user(new_user_name, registered_users)
#       st.write(f"You entered a new name: {new_user_name}")
# 
#   elif user_type == "Registered User":
#       selected_user = st.selectbox("Select Registered User", registered_users)
#       st.write(f"You selected the registered user: {selected_user}")
#   submit_button = st.button("Submit")
#   if submit_button:
#       if img_file_buffer is not None:
#           # To read the image file buffer as a PIL Image:
#           img_buffer = Image.open(img_file_buffer)
# 
#           # To convert PIL Image to a numpy array:
#           img_array = np.array(img_buffer)
# 
#           # Display the image with the selected user's name as the caption
#           if user_type == "New User":
#               st.image(img_array, caption=new_user_name)
#           elif user_type == "Registered User":
#               st.image(img_array, caption=selected_user)
# 
#           user_embed = embed_faces(img_array, model)
#           pid = ObjectId()
# 
#           facebank.insert_one({
#               'personId': pid,
#               'image': encode_image(img_array),
#               f'{embedding_path}': user_embed
#           })
# 
#           import pprint
#           db_result = pprint.pprint(facebank.find_one())
#           st.write(db_result)
#           # Check the shape of img_array:
#           # Should output shape: (height, width, channels)
# 
# def face_recognition():
#     st.title("Face Recognition")
#     img_file_buffer = st.camera_input("Take a photo to register")
#     submit_button = st.button("Submit")
#     #xử lí
#     st.image(img_file_buffer)
# 
# def main():
#     st.sidebar.title("Navigation")
#     app_mode = st.sidebar.selectbox("Choose the app mode", ["Register Face", "Face Recognition"])
# 
#     model, preprocess = load_model()
# 
#     if app_mode == "Register Face":
#         register_face()
#     elif app_mode == "Face Recognition":
#         face_recognition()
#

!streamlit run /content/gdrive/MyDrive/Advanced computer vision/Face recognition/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com